[project]
name = "{{ cookiecutter.project_slug }}"
version = "{{ cookiecutter.version }}"
description = "{{ cookiecutter.project_description }}"
authors = ["{{ cookiecutter.author_name }} <{{ cookiecutter.author_email }}>"]
channels = ["conda-forge"]
platforms = ["linux-64", "osx-64", "win-64"] # Add or remove as needed

[activation.env]

# --- MLflow Configuration ---
# Name of the MLflow experiment for this project
MLFLOW_PROJECT_NAME = "{{ cookiecutter.mlflow_project_name }}"
# URI of your MLflow tracking server
MLFLOW_TRACKING_URI = "{{ cookiecutter.mlflow_tracking_uri }}"

# --- Project Specific Paths (can be overridden) ---
# Root directory for all generated artifacts (DVC outputs, logs, configs)
# This is used by configure.py and other tasks.
ARTIFACTS_DIR = "{{ cookiecutter.artifacts_dir }}"
# Location of the main DVC pipeline file
DVC_FILE = "dvc.yaml"
# Location for the parent MLflow run ID
PIPELINE_ID_FILE = ".pipeline_id"

# --- Python Path ---
# Ensure src directory is on PYTHONPATH for imports like `from {{cookiecutter.project_slug}}.stages import ...`
PYTHONPATH = "${PYTHONPATH}:./src"


[tasks]
# ------------------------- Core Workflow Tasks -------------------------
# Configure the DVC pipeline from Hydra configs
configure = { cmd = "python configure.py", depends_on = ["clean-configs"] }

# Start a new pipeline run (generates .pipeline_id for MLflow)

# Run the full DVC pipeline (experiment tracking enabled)
# depends_on ensures configure and start_pipeline run first if not up-to-date.
pipeline = { cmd = "dvc exp run --set-param zendag.pipeline_id_file=$PIPELINE_ID_FILE", depends_on = ["configure", "start_pipeline"] }

# Save pipeline results (Git commit & push, DVC push)
# The confirm-gitadd task is a safety net.
save = { cmd = """
    echo "Committing changes and pushing data..." && \
    git add -A && \
    git commit --allow-empty -m "Save pipeline run results (MLflow Parent Run ID: $(cat $PIPELINE_ID_FILE 2>/dev/null || echo 'N/A'))" && \
    git push origin $(git branch --show-current) && \
    dvc push -r {{ cookiecutter.dvc_remote_name }}
    """, depends_on = ["ignore-dvc-outputs", "confirm-gitadd"] }

# ------------------------- Artifact Management Tasks -------------------------
# Clean only the composed Hydra configuration files in the artifacts directory
# This forces `configure` to regenerate them if it depends on this task.
clean-configs = "find $ARTIFACTS_DIR -type f -name '*.yaml' -delete && echo 'Cleaned composed configs from $ARTIFACTS_DIR.'"

# Clean all DVC-managed artifacts (requires DVC to be initialized)
# This removes outputs defined in dvc.yaml and their cache. Use with caution.
clean-dvc-artifacts = """
    echo 'This will remove all DVC-tracked outputs and their cache. Are you sure? (yes/N)' && \
    read answer && \
    if [ "$answer" != "yes" ]; then \
        echo 'Aborted by user.'; \
        exit 1; \
    fi && \
    dvc remove --outs $(dvc dag --short --downstream $DVC_FILE | cut -d' ' -f1 | grep -v '^$' | paste -sd ' ' -) && \
    dvc gc -w -f && \
    echo 'Cleaned DVC artifacts and cache.'
    """
# A more aggressive clean that removes the entire artifacts directory
# WARNING: This deletes EVERYTHING in ARTIFACTS_DIR, DVC tracked or not.
# It's useful for a complete reset but can lead to data loss if not careful.
# DVC will need to re-checkout/reproduce everything.
clean-artifacts-hard = """
    echo "WARNING: This will delete the entire '$ARTIFACTS_DIR' directory." && \
    echo "This includes DVC-tracked files (which might be recoverable from cache/remote if pushed)," && \
    echo "but also ANY untracked files in that directory." && \
    echo "Are you absolutely sure? (yes/N)" && \
    read answer && \
    if [ "$answer" != "yes" ]; then \
        echo 'Aborted by user.'; \
        exit 1; \
    fi && \
    rm -rf $ARTIFACTS_DIR && \
    mkdir -p $ARTIFACTS_DIR && \
    echo "Removed and recreated '$ARTIFACTS_DIR'."
    """

# Update .gitignore to ignore DVC-tracked output directories/files
# This uses dvc ls --dvc-only and creates/appends to $ARTIFACTS_DIR/.gitignore
# The main .gitignore should then ignore $ARTIFACTS_DIR/.gitignore itself or $ARTIFACTS_DIR/ if you never want to commit any artifact content
# (usually you only commit .dvc files for artifacts)
ignore-dvc-outputs = """
    echo "Updating $ARTIFACTS_DIR/.gitignore with DVC-tracked outputs..." && \
    mkdir -p $ARTIFACTS_DIR && \
    echo '# Auto-generated by `pixi run ignore-dvc-outputs`' > $ARTIFACTS_DIR/.gitignore && \
    echo '# These are DVC-tracked outputs. Add this file to your main .gitignore.' >> $ARTIFACTS_DIR/.gitignore && \
    dvc ls -R --dvc-only $ARTIFACTS_DIR | sed 's|^ARTIFACTS_DIR/||' >> $ARTIFACTS_DIR/.gitignore && \
    echo "Done. Ensure your main .gitignore includes '$ARTIFACTS_DIR/' or '$ARTIFACTS_DIR/.gitignore'."
    """

# ------------------------- Development & Utility Tasks -------------------------
# Run a single stage (for development/debugging) - requires ARTIFACTS_DIR, STAGE, and CN to be set
# Example: STAGE=training CN=train-model pixi run stage
# Format code using Ruff
fmt = "ruff format ."
cli = "./cli.sh"
# Lint code using Ruff
lint = "ruff check ."

# Run tests using pytest
test = "pytest"

# Safety check before committing: show large untracked files
confirm-gitadd = """
    echo 'Review large untracked files before committing:' && \
    (git ls-files --others --exclude-standard --directory | \
     xargs -I {} find {} -type f -size +1M -print0 2>/dev/null | \
     xargs -0 du -Lhs 2>/dev/null || echo 'No untracked files >1MB found.') && \
    python -c 'input("Confirm adding files to Git? (Press Enter to continue, Ctrl+C to cancel)")'
    """

[tasks.start_pipeline]
cmd = "python -m xp_workflow.start_pipeline run_name=$NAME project_name=$MLFLOW_PROJECT_NAME dvc_root=$(dvc root)"
env = {NAME = "default1"}

[tasks.stage]
cmd = "python -m zendag.run -cd $ARTIFACTS_DIR/$STAGE -cn $CN hydra.run.dir=$ARTIFACTS_DIR/$STAGE/$CN"
env = { STAGE = "", CN = "" } # User must set these

[system-requirements]
# cuda = "11.8" # Example: uncomment and specify if your project needs CUDA

[dependencies]
python = "={{ cookiecutter.python_version }}" # e.g. ==3.9.*
dvc = {extras = ["s3", "ssh"], version = "*"} # Example: dvc with s3 support, adjust as needed (e.g. gdrive, ssh)
xarray = "*"
gum = "*" # From conda-forge
dot = "*" # graphviz from conda-forge for dvc dag image rendering

{%- if cookiecutter.add_pytorch == "yes" -%}
[feature.gpu.dependencies]
pytorch-gpu = "*"

[feature.cpu.dependencies]
pytorch = "*"
{% endif %}


[pypi-dependencies]
{{cookiecutter.project_slug}} = { path = ".", editable = true }
zendag = "=={{ cookiecutter.zendag_version }}"
# Add any dependencies that are only available on PyPI and not conda-forge
# example-pypi-package = ">=1.0.0"

[feature.dev.dependencies]
# Dependencies for development (linters, formatters, testing)
ruff = ">=0.1.0"
pytest = ">=7.0"
pytest-cov = ">=4.0"
pre-commit = ">=3.0" # For git hooks
mypy = ">=1.0" # Optional: for static type checking

[feature.notebook.dependencies]
# Dependencies for Jupyter notebooks/labs
ipykernel = "*"
# Add plotting libraries etc.
matplotlib = "*"
hvplot = "*"

{%- if cookiecutter.add_pytorch == "yes" -%}
lovely-tensors = "*"
{% endif %}

# seaborn = "*"

[feature.notebook.activation.env]
LOVELY_TENSORS = "1"
BETTER_EXCEPTIONS = "1"
FORCE_COLOR="1"

[environments]
{%- if cookiecutter.add_pytorch == "yes" -%}
default = ["dev", "gpu"] # Default environment includes 'dev' features
no-dev = ["gpu"] 
no-dev-cpu = ["cpu"] 
notebook = { features = ["notebook", "dev", "gpu"], solve-group = "notebook" }
notebook-cpu = { features = ["notebook", "dev", "gpu"], solve-group = "notebook" }
{%- elif cookiecutter.add_pytorch == "no" -%}

default = ["dev"] # Default environment includes 'dev' features
no-dev = [] 
notebook = { features = ["notebook", "dev"], solve-group = "notebook" }
{% endif %}
lint = { features = ["dev"], no-default-feature = true, solve-group = "lint" }