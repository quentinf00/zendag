# {{ cookiecutter.project_name }}

This project was generated from the ZenFlow Cookiecutter template. It provides a starting point for building reproducible ML experiments using ZenFlow, Hydra, DVC, MLflow, and Pixi.

## Prerequisites

*   [Pixi](https://pixi.sh/latest/installation/): For environment and task management.
*   [Cookiecutter](https://cookiecutter.readthedocs.io/en/stable/installation.html): To generate the project from the template.
*   Access to an MLflow tracking server.

## Getting Started

1.  **Generate the Project:**
    ```bash
    cookiecutter gh:your-username/zenflow-cookiecutter # Or use a local path
    # Follow the prompts for project name, etc.
    ```

2.  **Navigate to Project Directory:**
    ```bash
    cd {{ cookiecutter.project_slug }}
    ```

3.  **Install Environment & Dependencies:**
    ```bash
    pixi install
    ```
    This installs all dependencies defined in `pixi.toml` into a virtual environment managed by Pixi.

4.  **Activate Environment:**
    ```bash
    pixi shell
    ```
    You are now inside the project's environment.

5.  **Configure MLflow:**
    *   Set the `MLFLOW_TRACKING_URI` environment variable. You can:
        *   Modify the `[activation.env]` section in `pixi.toml`.
        *   Set it in your shell environment (`export MLFLOW_TRACKING_URI=...`).
        *   Create a `.env` file (ensure it's in `.gitignore`).
    *   Optionally, update `MLFLOW_PROJECT_NAME` in `pixi.toml` or via environment variables.

## Project Structure Overview

*   **`pixi.toml`:** Defines Python/CUDA versions, dependencies (like `zenflow`, `hydra-core`, `dvc`, `mlflow`), and project tasks (`pixi run <task>`).
*   **`configure.py`:** A script that uses `zenflow.configure_pipeline` to read configurations from `configs/` and generate `dvc.yaml`. You'll edit this to add new stages.
*   **`dvc.yaml`:** The pipeline definition file **generated by `pixi run configure`**. Do not edit this manually unless absolutely necessary.
*   **`configs/`:** Define your experiment configurations here using Hydra-Zen. Group related configs into files (e.g., `data.py`, `model.py`). Use `zenflow.deps_path` and `zenflow.outs_path` to declare file dependencies/outputs within these configs.
*   **`src/{{ cookiecutter.project_slug }}/`:** Your project's Python source code.
    *   `stages/`: Implement the logic for each DVC pipeline stage here. Decorate main stage functions with `@zenflow.mlflow_run`.
*   **`artifacts/`:** Directory where DVC-tracked outputs (composed configs, logs, models, checkpoints, results) are stored by default. Managed by DVC.
*   **`.pipeline_id`:** A file automatically managed by `@zenflow.mlflow_run` to track the parent MLflow run for a pipeline execution.

## Core Workflow

1.  **Define Configs:** Add or modify Hydra-Zen configurations in the `configs/` directory. Use `deps_path` and `outs_path` for file paths that are inputs/outputs of stages.
2.  **Implement Stages:** Write the Python functions for your pipeline stages in `src/{{ cookiecutter.project_slug }}/stages/`. Decorate them with `@zenflow.mlflow_run`.
3.  **Register Stages:** In `configure.py`:
    *   Import your configurations from `configs/`.
    *   Add them to the `hydra_zen.ZenStore`.
    *   Ensure the relevant `stage_group` names are included in the list passed to `zenflow.configure_pipeline`.
4.  **Generate/Update Pipeline:**
    ```bash
    pixi run configure
    ```
    This reads your configs and generates/updates `dvc.yaml`. Check this file into Git.
5.  **Run Pipeline:**
    ```bash
    pixi run pipeline [--dvc-args="-f"] # Add --dvc-args="..." to pass args like -f to dvc
    ```
    This executes the DVC pipeline (`dvc exp run` by default). Each stage will:
    *   Start a nested MLflow run.
    *   Log parameters from its Hydra config.
    *   Execute your stage function.
    *   Log artifacts (config, logs).
6.  **Analyze Results:** Use the MLflow UI (connected to your `MLFLOW_TRACKING_URI`) to compare runs, view metrics, parameters, and artifacts.
7.  **Save Results:** If satisfied with a run:
    ```bash
    pixi run save-pipeline # Optional: NAME=<branch_or_tag_name> pixi run save-pipeline
    ```
    This commits code changes (including `dvc.yaml` and `dvc.lock`) to Git and pushes DVC-tracked data/artifacts to remote storage. *Check Git status carefully before running.*

## Adding a New Stage (Example: `evaluate`)

1.  **Write Stage Function:** Create `src/{{ cookiecutter.project_slug }}/stages/evaluate.py`:
    ```python
    from zenflow.mlflow_utils import mlflow_run
    from omegaconf import DictConfig
    import pandas as pd
    import logging

    log = logging.getLogger(__name__)

    @mlflow_run
    def evaluate_model(cfg: DictConfig):
        log.info(f"Loading predictions from: {cfg.predictions_file}")
        preds_df = pd.read_csv(cfg.predictions_file)
        # ... calculate metrics ...
        log.info(f"Calculated metric: {metric_value}")
        mlflow.log_metric("final_score", metric_value)
        # Maybe save metrics to a file declared as an output
        with open(cfg.metrics_output_file, 'w') as f:
            f.write(f"final_score: {metric_value}\n")
        log.info(f"Saved metrics to: {cfg.metrics_output_file}")

    # Make sure logging is configured if running directly
    if __name__ == "__main__":
        logging.basicConfig(level=logging.INFO)
        # Minimal config for direct run example (normally run via hydra/dvc)
        mock_cfg = OmegaConf.create({
            "predictions_file": "path/to/dummy_preds.csv", # Needs actual file for real run
            "metrics_output_file": "path/to/dummy_metrics.txt"
        })
        # evaluate_model(mock_cfg) # Uncomment to test directly
    ```

2.  **Define Config:** Create/edit `configs/evaluate.py`:
    ```python
    from hydra_zen import builds, store
    from zenflow.config_utils import deps_path, outs_path
    # Assuming your evaluate_model function is importable
    from {{ cookiecutter.project_slug }}.stages.evaluate import evaluate_model

    # Define stage dir function if needed for deps_path resolution
    # from zenflow.core import default_stage_dir_fn as stage_dir_fn

    EvaluateConfig = builds(
        evaluate_model,
        populate_full_signature=True, # Good practice
        # Declare dependencies and outputs using paths relative to stage dirs
        predictions_file=deps_path("predictions.csv", input_stage="predict", input_name="predict_on_test"), # Example dep
        metrics_output_file=outs_path("evaluation_metrics.txt"),
        # zen_meta=dict(stage_dir_fn=stage_dir_fn) # Pass if needed by deps_path
    )

    # Add to a store group (e.g., 'evaluate')
    eval_store = store(group="evaluate")
    eval_store(EvaluateConfig, name="eval_default")
    ```

3.  **Register in `configure.py`:**
    ```python
    # configure.py
    import hydra_zen
    # Import your new config definitions
    import configs.evaluate # This executes the code in evaluate.py, adding to the store

    # ... other imports ...
    from zenflow.core import configure_pipeline

    # Ensure the 'evaluate' store group exists (done by importing configs.evaluate)
    store = hydra_zen.store # Access the global default store

    # Define all stage groups to be processed in order (usually matters for DVC)
    STAGE_GROUPS = [
        "data_prep",
        "feature_eng",
        "training",
        "predict",
        "evaluate", # Add the new stage group
    ]

    if __name__ == "__main__":
        # Basic logging setup for configure script itself
        import logging
        logging.basicConfig(level=logging.INFO, format='%(levelname)s:%(name)s:%(message)s')

        configure_pipeline(store=store, stage_groups=STAGE_GROUPS)
        print("Configuration finished. 'dvc.yaml' generated/updated.")

    ```

4.  **Run `configure`:**
    ```bash
    pixi run configure
    ```
    Check `dvc.yaml` to see the new `evaluate/eval_default` stage.

5.  **Run the New Stage (or full pipeline):**
    ```bash
    dvc repro evaluate/eval_default # Run only the new stage and its dependencies
    # OR
    pixi run pipeline # Run the whole pipeline from the start
    ```

## Customization

*   **Tasks:** Modify or add tasks in `pixi.toml` (e.g., custom plotting scripts, deployment tasks).
*   **Dependencies:** Add Python packages or system requirements in `pixi.toml`. Run `pixi add <package>` or edit manually and run `pixi install`.
*   **Pipeline Structure:** Modify `STAGE_GROUPS` in `configure.py` and adjust configuration dependencies (`deps_path`, `outs_path`) to change the pipeline flow.