# -*- coding: utf-8 -*-
"""zendag_demo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PQiJYh4-sp4HWT90TiQCG4lioIXGLKrw
"""

!pip install --quiet dvc

pip install --quiet zendag

!git config --global user.email "you@example.com"
!git config --global user.name "Your Name"
!git config --global init.defaultBranch main

!git init
!dvc init

# Commented out IPython magic to ensure Python compatibility.
# %%writefile fns.py
# import pandas as pd
# import numpy as np
# from pathlib import Path
# import matplotlib.pyplot as plt
# 
# 
# def generate(num_points: int, output_path: str, seed: int):
#     """Generates a simple dataset with x and y columns."""
#     np.random.seed(seed)
#     Path(output_path).parent.mkdir(parents=True, exist_ok=True)
#     x = np.linspace(0, 10, num_points)
#     y = 2 * x + np.random.normal(0, 1, num_points) + (num_points / 10) # Add offset based on num_points
#     df = pd.DataFrame({'x': x, 'y': y})
#     df.to_csv(output_path, index=False)
#     print(f"Generated data with {num_points} points to {output_path}")
# 
# def transform(input_path: str, output_path: str, scale_factor: float):
#     """Transforms the 'y' column by a scale factor."""
#     Path(output_path).parent.mkdir(parents=True, exist_ok=True)
#     df = pd.read_csv(input_path)
#     df['y'] = df['y'] * scale_factor
#     df.to_csv(output_path, index=False)
#     print(f"Transformed data from {input_path} with scale {scale_factor} to {output_path}")
# 
# def combine(input_path_a: str, input_path_b: str, output_path: str, source_label_suffix: str):
#     """Combines two datasets by adding a source column and concatenating."""
#     Path(output_path).parent.mkdir(parents=True, exist_ok=True)
#     df_a, df_b = pd.read_csv(input_path_a), pd.read_csv(input_path_b)
# 
#     df_a['source'] = f'A_{source_label_suffix}'
#     df_b['source'] = f'B_{source_label_suffix}'
# 
# 
#     combined_df = pd.concat([df_a, df_b], ignore_index=True)
#     combined_df.to_csv(output_path, index=False)
#     print(f"Combined data from {input_path_a} and {input_path_b} with suffix '{source_label_suffix}' to {output_path}")
# 
# 
# def plot(input_path: str, output_path: str, plot_title: str, plot_col="y"):
#     """Plots the combined data."""
#     Path(output_path).parent.mkdir(parents=True, exist_ok=True)
#     df = pd.read_csv(input_path)
#     fig, ax = plt.subplots(figsize=(10, 6))
#     df.plot(kind='scatter', x='x', y=plot_col, ax=ax, s=50, color=pd.factorize(df['source'])[0], cmap='viridis')
#     fig.savefig(output_path)
#     plt.close()
#     print(f"Plot saved to {output_path} with title '{plot_title}'")
#

import fns
from pathlib import Path
xp_dir = Path("xps/xp0")
xp_dir.mkdir(exist_ok=True, parents=True)


fns.generate(
    num_points=100,
    output_path=xp_dir / "data.csv",
    seed=42
)
fns.transform(
    input_path=xp_dir / "data.csv",
    output_path=xp_dir / "transformed.csv",
    scale_factor=2
)
fns.combine(
    input_path_a=xp_dir / "data.csv",
    input_path_b=xp_dir / "transformed.csv",
    output_path=xp_dir / "combined.csv",
    source_label_suffix="original"
)
fns.plot(
    input_path=xp_dir / "combined.csv",
    output_path=xp_dir / "plot.png",
    plot_title="Combined Data"
)

"""## Configure with hydra_zen"""

import logging
import sys

handler = logging.StreamHandler(sys.stdout)
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
handler.setFormatter(formatter)

root_logger = logging.getLogger()
root_logger.setLevel(logging.DEBUG)  # Set your desired logging level
root_logger.handlers = [handler]     # Replace handlers with your stdout handler

# Commented out IPython magic to ensure Python compatibility.
# %%writefile dirs.py
# from pathlib import Path
# xp = "xp2"
# xp_dir = Path("xps") / xp
# 
# def configs_dir_fn(name): return xp_dir / 'cfgs'
# def stage_dir_fn(groug, name): return xp_dir
# 
# DIRS = lambda xp: {
#     "configs_dir_fn": (lambda name: Path("xps") / xp / 'cfgs'),
#     "stage_dir_fn": (lambda groug, name: Path("xps") / xp),
# }

import hydra_zen
import zendag
from zendag import deps_path, outs_path, configure_pipeline
from functools import partial
from pathlib import Path
import fns


def xp_dir_fn(xp): return Path("xps") / xp
DIRS = lambda xp: {
    "configs_dir_fn": (lambda name: xp_dir_fn(xp) / 'cfgs'),
    "stage_dir_fn": (lambda groug, name: xp_dir_fn(xp)),
}

XP =  2
xp_dir = xp_dir_fn(XP)

outs = partial(zendag.outs_path, root_dir=True)
store = hydra_zen.ZenStore(overwrite_ok=True)

store(
    fns.generate,
    num_points=100,
    output_path=outs( xp_dir /"data.csv"),
    seed=42,
)
store(
    fns.transform,
    input_path=deps_path(xp_dir / "data.csv"),
    output_path=outs(xp_dir / "transformed.csv"),
    scale_factor=2,

)
store(
    fns.combine,
    input_path_a=deps_path(xp_dir / "data.csv",),
    input_path_b=deps_path(xp_dir / "transformed.csv"),
    output_path=outs(xp_dir / "combined.csv"),
    source_label_suffix="original",

)
store(
    fns.plot,
    input_path=deps_path(xp_dir / "combined.csv"),
    output_path=outs(xp_dir / "plot.png"),
    plot_title="Combined Data",

)
zendag.configure_pipeline(
    store,
    stage_groups=[None],
    configs_dir_fn=dirs.configs_dir_fn,
    stage_dir_fn=dirs.stage_dir_fn,

)

print(hydra_zen.to_yaml(store[None, 'generate']))

!git add dvc.yaml fns.py dirs.py
!git commit -m "First dag"

!dvc exp run

!dvc exp run

# Commented out IPython magic to ensure Python compatibility.
# %%writefile track.py
# from omegaconf import OmegaConf
# from pathlib import Path
# import mlflow
# import dirs
# import logging
# import pandas as pd
# import zendag
# 
# _log = logging.getLogger(__name__)
# 
# @zendag.mlflow_run(**dirs.KWS)
# def track_xp(xp_dir):
#     xp_dir = Path(xp_dir)
#     _log.info(f"Logging from {xp_dir}")
# 
#     for cfg in xp_dir.glob("cfgs/*.yaml"):
#         _log.info(f"Logging {cfg}")
#         mlflow.log_artifact(cfg, 'cfgs')
#         _log.info(f"Logging params from {cfg}")
# 
#         d = pd.json_normalize(
#             {cfg.stem: OmegaConf.to_container(
#                  OmegaConf.load(cfg), resolve=True
#             )},  # Resolve interpolations before logging
#             sep=".",
#         ).to_dict(orient="records")[0]
#         mlflow.log_params(d)
#     _log.info(f"Logging plot from {xp_dir / 'plot.png'}")
#     mlflow.log_artifact(xp_dir / 'plot.png')

import track
import zendag
import importlib
importlib.reload(zendag)
importlib.reload(track)

import track
import dirs
from functools import partial
from dirs import xp_dir


store(
    track.track_xp,
    xp_dir=deps_path(xp_dir)
)
configure_pipeline(
    store,
    stage_groups=[None],
    **dirs.KWS
)

!MLFLOW_PROJECT_NAME=MYZENDAGXP dvc exp run



!rm -f mlruns.zip
!zip -r  mlruns.zip mlruns

