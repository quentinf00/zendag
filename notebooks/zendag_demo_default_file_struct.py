# -*- coding: utf-8 -*-
"""zendag_demo_default_file_struct.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LIziVR78W_4d8HwZT24GtBakuksaK5vo
"""



"""from types import WrapperDescriptorType
- have different xps folder and share stage execution cache by setting the wdir of the dvc run
- workflow: default paths used
    artifacts/<xpname>/{<stage_cfg>.yml, <all_artifacts>.dat}
- set dvc stages wdir
- need to install current project as a python package
"""

!pip install --quiet dvc

pip install --quiet git+https://github.com/quentinf00/zendag.git#egg=zendag

!git config --global user.email "you@example.com"
!git config --global user.name "Your Name"
!git config --global init.defaultBranch main

!git init
!dvc init

# Commented out IPython magic to ensure Python compatibility.
# %%writefile pyproject.toml
# 
# [project]
# name = "zendagdemo"
# version = "0.0.1" # Add a version
# 
# [tool.setuptools]
# packages = ["zendagdemo"]
# 
#

!mkdir -p zendagdemo
!touch zendagdemo/__init__.py
!pip install -e .



# Commented out IPython magic to ensure Python compatibility.
# %%writefile zendagdemo/fns.py
# import pandas as pd
# import numpy as np
# from pathlib import Path
# import matplotlib.pyplot as plt
# 
# 
# def generate(num_points: int, output_path: str, seed: int):
#     """Generates a simple dataset with x and y columns."""
#     np.random.seed(seed)
#     Path(output_path).parent.mkdir(parents=True, exist_ok=True)
#     x = np.linspace(0, 10, num_points)
#     y = 2 * x + np.random.normal(0, 1, num_points) + (num_points / 10) # Add offset based on num_points
#     df = pd.DataFrame({'x': x, 'y': y})
#     df.to_csv(output_path, index=False)
#     print(f"Generated data with {num_points} points to {output_path}")
# 
# def transform(input_path: str, output_path: str, scale_factor: float):
#     """Transforms the 'y' column by a scale factor."""
#     Path(output_path).parent.mkdir(parents=True, exist_ok=True)
#     df = pd.read_csv(input_path)
#     df['y'] = df['y'] * scale_factor
#     df.to_csv(output_path, index=False)
#     print(f"Transformed data from {input_path} with scale {scale_factor} to {output_path}")
# 
# def combine(input_path_a: str, input_path_b: str, output_path: str, source_label_suffix: str):
#     """Combines two datasets by adding a source column and concatenating."""
#     Path(output_path).parent.mkdir(parents=True, exist_ok=True)
#     df_a, df_b = pd.read_csv(input_path_a), pd.read_csv(input_path_b)
# 
#     df_a['source'] = f'A_{source_label_suffix}'
#     df_b['source'] = f'B_{source_label_suffix}'
# 
# 
#     combined_df = pd.concat([df_a, df_b], ignore_index=True)
#     combined_df.to_csv(output_path, index=False)
#     print(f"Combined data from {input_path_a} and {input_path_b} with suffix '{source_label_suffix}' to {output_path}")
# 
# 
# def plot(input_path: str, output_path: str, plot_title: str, plot_col="y"):
#     """Plots the combined data."""
#     Path(output_path).parent.mkdir(parents=True, exist_ok=True)
#     df = pd.read_csv(input_path)
#     fig, ax = plt.subplots(figsize=(10, 6))
#     df.plot(kind='scatter', x='x', y=plot_col, ax=ax, s=50, color=pd.factorize(df['source'])[0], cmap='viridis')
#     fig.savefig(output_path)
#     plt.close()
#     print(f"Plot saved to {output_path} with title '{plot_title}'")
#



import zendagdemo.fns as fns
from pathlib import Path
xp_dir = Path("xps/xp0")
xp_dir.mkdir(exist_ok=True, parents=True)


fns.generate(
    num_points=100,
    output_path=xp_dir / "data.csv",
    seed=42
)
fns.transform(
    input_path=xp_dir / "data.csv",
    output_path=xp_dir / "transformed.csv",
    scale_factor=2
)
fns.combine(
    input_path_a=xp_dir / "data.csv",
    input_path_b=xp_dir / "transformed.csv",
    output_path=xp_dir / "combined.csv",
    source_label_suffix="original"
)
fns.plot(
    input_path=xp_dir / "combined.csv",
    output_path=xp_dir / "plot.png",
    plot_title="Combined Data"
)

"""## Configure with hydra_zen"""

import logging
import sys

handler = logging.StreamHandler(sys.stdout)
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
handler.setFormatter(formatter)

root_logger = logging.getLogger()
root_logger.setLevel(logging.DEBUG)  # Set your desired logging level
root_logger.handlers = [handler]     # Replace handlers with your stdout handler

import hydra_zen
import zendag
from zendag import deps_path, outs_path, configure_pipeline
from functools import partial
from pathlib import Path
import zendagdemo.fns as fns


XP="xp1"
outs = partial(zendag.outs_path)
store = hydra_zen.ZenStore(overwrite_ok=True)
xp_store = store(group=XP)
deps = partial(zendag.deps_path, input_stage=XP)

xp_store(
    fns.generate,
    num_points=100,
    output_path=outs("data.csv"),
    seed=42,
)
xp_store(
    fns.transform,
    input_path=deps("data.csv", input_name='generate'),
    output_path=outs("transformed.csv"),
    scale_factor=2,

)
xp_store(
    fns.combine,
    input_path_a=deps("data.csv", input_name="generate"),
    input_path_b=deps( "transformed.csv", input_name="transform",),
    output_path=outs("combined.csv"),
    source_label_suffix="original",

)
xp_store(
    fns.plot,
    input_path=deps("combined.csv", input_name="combine"),
    output_path=outs("plot.png"),
    plot_title="Combined Data",
)

zendag.configure_pipeline(
    xp_store,
    stage_groups=[XP],
    wdir=f'artifacts/{XP}',
    dvc_stage_name_fn=(lambda _,n :n)
)

print(hydra_zen.to_yaml(store[XP, 'generate']))

!git add dvc.yaml zendagdemo
!git commit -m "First dag"

!dvc exp run

!HYDRA_FULL_ERROR=1 dvc exp run

# Commented out IPython magic to ensure Python compatibility.
# %%writefile zendagdemo/track.py
# from omegaconf import OmegaConf
# from pathlib import Path
# import mlflow
# import logging
# import pandas as pd
# import zendag
# 
# _log = logging.getLogger(__name__)
# 
# @zendag.mlflow_run()
# def track_xp(xp_dir):
#     xp_dir = Path(xp_dir)
#     _log.info(f"Logging from {xp_dir}")
# 
#     for cfg in xp_dir.glob("*.yaml"):
#         _log.info(f"Logging {cfg}")
#         mlflow.log_artifact(cfg, 'cfgs')
#         _log.info(f"Logging params from {cfg}")
# 
#         d = pd.json_normalize(
#             {cfg.stem: OmegaConf.to_container(
#                  OmegaConf.load(cfg), resolve=True
#             )},  # Resolve interpolations before logging
#             sep=".",
#         ).to_dict(orient="records")[0]
#         mlflow.log_params(d)
#     _log.info(f"Logging plot from {xp_dir / 'plot' / 'plot.png'}")
#     mlflow.log_artifact(xp_dir /  'plot' / 'plot.png')

import track
import zendag
import importlib
importlib.reload(zendag)
importlib.reload(track)

import track


xp_store(
    track.track_xp,
    xp_dir=deps_path('', XP, '.')
)
configure_pipeline(
    xp_store,
    stage_groups=[XP],
)

!MLFLOW_PROJECT_NAME=MYZENDAGXP dvc exp run



!rm -f mlruns.zip
!zip -r  mlruns.zip mlruns

